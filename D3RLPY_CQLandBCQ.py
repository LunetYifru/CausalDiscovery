"""Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1i-Yw6JHvFf8e_W3BAXOwL8-SNSjbjD7g
"""
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from d3rlpy.algos import DiscreteBCQ
import matplotlib
import d3rlpy
from d3rlpy.metrics.scorer import discrete_action_match_scorer
from d3rlpy.ope import DiscreteFQE
import torch
import matplotlib.pyplot as plot
from d3rlpy.algos import DiscreteCQL
from d3rlpy.algos import DiscreteBCQ
from numpy import testing
from d3rlpy.dataset import MDPDataset
import numpy as np
import pandas as pd


df1 = pd.read_csv('Errors_dataset_combined.csv')
df2 = pd.read_csv('Test_dataset.csv')

"""Actions"""


def randaction(dataset):
    import random
    i = 0
    RandAction = []
    for i in range(len(dataset)):
        RandAction.append(random.randint(0, 1))

    # dataset['RandAction'] = RandAction
    dataset['RandAction'] = dataset['LABEL_ERROR_STATE']

    n = 0
    k = 0

    for i in dataset['RandAction']:
        if i == 0:
            n += 1
        else:
            k += 1

    print(n, k)

    return dataset


df_train = randaction(df1)
df_test = randaction(df2)

df_test.head()

"""Rewards"""


def reward(dataset):
    reward = []
    for i in range(len(dataset)):
        if dataset['RandAction'][i] == dataset['LABEL_ERROR_STATE'][i]:
            reward.append(1)
        else:
            reward.append(0)

    dataset['Reward'] = reward
    return dataset


df_train_W_rewards = reward(df_train)
df_test_W_rewards = reward(df_test)
df_train_W_rewards.head()


def adjustTerminals(dataset):
    terminals = []
    for i in range(1, len(dataset)):
        if dataset.iloc[i]['Time'] != '0:00':
            terminals.append(0)
        else:
            terminals.append(1)
    terminals.append(1)

    dataset['Terminals'] = terminals
    return dataset


df_train_final = adjustTerminals(df_train_W_rewards)
df_test_final = adjustTerminals(df_test_W_rewards)

df_test_final.head(1440)

i = 1440
if df_test_final.iloc[i-1]['Time'] != '0:00':
    print(df_test_final.iloc[i-1])

df_test_final.head()

States = df_train_final.drop(
    columns=['Time', 'LABEL_ERROR_STATE', 'RandAction', 'Reward', 'Terminals'])
Actions = df_train_final['RandAction']
Rewards = df_train_final['Reward']
Terminals = df_train_final['Terminals']

States2 = df_test_final.drop(
    columns=['Time', 'LABEL_ERROR_STATE', 'RandAction', 'Reward', 'Terminals'])
Actions2 = df_test_final['LABEL_ERROR_STATE']
Rewards2 = df_test_final['Reward']
Terminals2 = df_test_final['Terminals']

print(len(States), len(Actions), len(Terminals), len(Rewards))
print(len(States2), len(Actions2), len(Terminals2), len(Rewards2))


###########Training MDP Dataset####################
observations = States.to_numpy()
actions = Actions.to_numpy()
rewards = Rewards.to_numpy()
terminals = Terminals.to_numpy()

dataset_train = MDPDataset(observations, actions,
                           rewards, terminals, discrete_action=True)

###########Test MDP Dataset####################
observations2 = States2.to_numpy()
actions2 = Actions2.to_numpy()
rewards2 = Rewards2.to_numpy()
terminals2 = Terminals2.to_numpy()

dataset_test = MDPDataset(observations2, actions2,
                          rewards2, terminals2, discrete_action=True)

episodes_train = dataset_train.episodes
episodes_test = dataset_test.episodes

print(len(episodes_train), len(episodes_test))

# episode = dataset.episodes
# episode.transitions
# len(episode[0].observations)
# # episode[0].actions
# # episode[0].next_rewards
# # episode[0].next_observations
# # episode[0].terminals

# transition = episodes[0]
# while transition.next_transition:
#     transition = transition.next_transition

# n=0
# for i in episodes_train:
#   if i.terminal == 1:
#     n+= 1
# print(n)

# episodes_test[1].terminal

"""Training with CQL"""
epochs = [50, 50, 50, 50, 50]
for i in epochs:
    cql = DiscreteCQL(use_gpu=False, batch_size=64, gamma=0.99)
    cql.fit(episodes_train, n_epochs=i, show_progress=True)

    cql.save_model(fname='DiscreteCQLModel')

    """Greedy actions prediction"""

    actions = np.empty(0)
    for n in episodes_test:
        a = cql.predict(n.observations)
        actions = np.append(actions, a)

    print(actions[:1000])
    print(actions2[:1000])

    correct = 0

    for b in range(0, len(actions)):
        if actions[b] == actions2[b]:
            correct += 1

    print(correct*100/len(actions))

    """Evaluation using FQE - CQL"""

    # off-policy evaluation algorithm
    fqe = DiscreteFQE(algo=cql)

    fqe.fit(episodes_test,
            eval_episodes=episodes_test,
            n_epochs=5,
            scorers={
                'discrete_action_match': discrete_action_match_scorer,
            })


"""Training with BCQL"""
epochs = [50, 50, 50, 50, 50]

for i in epochs:

    bcq = DiscreteBCQ(use_gpu=False, batch_size=64, gamma=0.99)
    bcq.fit(episodes_train, n_epochs=i, show_progress=True)
    bcq.save_model(fname='DiscreteBCQLModel')

    actions = np.empty(0)
    for n in episodes_test:
        a = bcq.predict(n.observations)
        actions = np.append(actions, a)

    print(actions[:1000])
    print(actions2[:1000])

    correct = 0

    for b in range(0, len(actions)):
        if actions[b] == actions2[b]:
            correct += 1

    print(correct*100/len(actions))

    """Evaluation using FQE BCQ"""

    # off-policy evaluation algorithm
    fqe = DiscreteFQE(algo=bcq)

    # metrics to evaluate with

    fqe.fit(episodes_test,
            eval_episodes=episodes_test,
            n_epochs=10,
            scorers={
                'discrete_action_match': discrete_action_match_scorer,
            })

# # create algorithm with saved configuration
# cql = cql.from_json('downloads/d3rlpy_logs/params.json')
# cql.load_model('Downloads/d3rlpy_logs/DiscreteCQLModel.pt')

# # save greedy-policy as TorchScript
# cql.save_policy('policy.pt')

# # save greedy-policy as ONNX
# cql.save_policy('policy.onnx')

# # load greedy-policy only with PyTorch
# policy = torch.jit.load('policy.pt')